{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ef10fd-c34f-48f1-992b-8bddfaae0da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "from minet.notebook import run_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "313dc44b-ad03-4a32-9856-6663bcbef38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KFetching \u001b[38;5;237m━━━━━━━━━━\u001b[0m    235/10,003 urls ⠧ [  2%] in 25.43s (9.24/s eta: 17:40.0)   ^C\n",
      "\u001b[2KFetching \u001b[38;5;237m━━━━━━━━━━\u001b[0m    239/10,003 urls ⠙ [  2%] in 25.68s (9.31/s eta: 17:27.0) \u001b[1A\n",
      "\u001b[?25hPerforming clean shutdown by cancelling ongoing calls...\n",
      "This may take some seconds if you are hitting slow servers.\n",
      "Ctrl-C again if you want to force exit.\n"
     ]
    }
   ],
   "source": [
    "!cd ../../.. && python -m minet.cli fetch url -i ftest/resources/urls.csv -D --simple-progress -o /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7fb6712-29df-414c-9c93-3a222f40aa42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KScraping \u001b[38;5;237m━━━━━━━━━━\u001b[0m 0/1 queries ⠇ - in 8.70s (?/s) total: 37 tweets               ^C\n",
      "\u001b[2KScraping \u001b[38;5;237m━━━━━━━━━━\u001b[0m 0/1 queries ⠋ - in 8.87s (?/s) total: 37 tweets             \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!cd ../../.. && python -m minet.cli tw scrape tweets test --simple-progress -o /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2457c91-a354-42dd-9740-82cb661a19ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2KParsing \u001b[32m━━━━━━━━━━\u001b[0m 10,003/10,003 urls · [100%] in 3.94s (2.54k/s)               .00s)    s)    .00s)    s)    \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!cd ../../.. && python -m minet.cli url-parse url -i ftest/resources/urls.csv --simple-progress -o /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b37ed91d-edfe-4b8b-b2cc-2b44f842a8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;250;194;43mUsage:\u001b[0m minet scrape [\u001b[36m-h\u001b[0m] [\u001b[36m--silent\u001b[0m] [\u001b[36m--refresh-per-second\u001b[0m \u001b[38;5;36mREFRESH_PER_SECOND\u001b[0m]\n",
      "                    [\u001b[36m--simple-progress\u001b[0m] [\u001b[36m-g\u001b[0m] [\u001b[36m-I\u001b[0m \u001b[38;5;36mINPUT_DIR\u001b[0m] [\u001b[36m-p\u001b[0m \u001b[38;5;36mPROCESSES\u001b[0m]\n",
      "                    [\u001b[36m--chunk-size\u001b[0m \u001b[38;5;36mCHUNK_SIZE\u001b[0m] [\u001b[36m--body-column\u001b[0m \u001b[38;5;36mBODY_COLUMN\u001b[0m]\n",
      "                    [\u001b[36m--url-column\u001b[0m \u001b[38;5;36mURL_COLUMN\u001b[0m] [\u001b[36m--error-column\u001b[0m \u001b[38;5;36mERROR_COLUMN\u001b[0m]\n",
      "                    [\u001b[36m--status-column\u001b[0m \u001b[38;5;36mSTATUS_COLUMN\u001b[0m]\n",
      "                    [\u001b[36m--encoding-column\u001b[0m \u001b[38;5;36mENCODING_COLUMN\u001b[0m]\n",
      "                    [\u001b[36m--mimetype-column\u001b[0m \u001b[38;5;36mMIMETYPE_COLUMN\u001b[0m] [\u001b[36m--encoding\u001b[0m \u001b[38;5;36mENCODING\u001b[0m]\n",
      "                    [\u001b[36m-f\u001b[0m \u001b[38;5;36m{csv,jsonl,ndjson}\u001b[0m]\n",
      "                    [\u001b[36m--plural-separator\u001b[0m \u001b[38;5;36mPLURAL_SEPARATOR\u001b[0m] [\u001b[36m--strain\u001b[0m \u001b[38;5;36mSTRAIN\u001b[0m]\n",
      "                    [\u001b[36m-i\u001b[0m \u001b[38;5;36mINPUT\u001b[0m] [\u001b[36m--explode\u001b[0m \u001b[38;5;36mEXPLODE\u001b[0m] [\u001b[36m-s\u001b[0m \u001b[38;5;36mSELECT\u001b[0m] [\u001b[36m--total\u001b[0m \u001b[38;5;36mTOTAL\u001b[0m]\n",
      "                    [\u001b[36m-o\u001b[0m \u001b[38;5;36mOUTPUT\u001b[0m]\n",
      "                    \u001b[36mscraper\u001b[0m \u001b[36m[filename_or_filename_column]\u001b[0m\n",
      "\n",
      "\u001b[1;38;2;212;42;32m# Minet Scrape Command\u001b[0m\n",
      "\n",
      "\u001b[39mUse multiple processes to scrape data from a batch of HTML files using\u001b[0m\n",
      "\u001b[39mminet scraping DSL documented here:\u001b[0m\n",
      "\u001b[36mhttps://github.com/medialab/minet/blob/master/docs/cookbook/scraping_dsl.md\u001b[0m\n",
      "\n",
      "\u001b[39mIt will output the scraped items as a CSV or NDJSON file.\u001b[0m\n",
      "\n",
      "\u001b[39mNote that this command has been geared towards working in tandem with\u001b[0m\n",
      "\u001b[39mthe fetch command. This means the command expects, by default, CSV files\u001b[0m\n",
      "\u001b[39mcontaining columns like \"\u001b[0m\u001b[38;5;36mfilename\u001b[0m\u001b[39m\", \"\u001b[0m\u001b[38;5;36mhttp_status\u001b[0m\u001b[39m\", \"\u001b[0m\u001b[38;5;36mencoding\u001b[0m\u001b[39m\" etc. as\u001b[0m\n",
      "\u001b[39myou can find in a fetch command CSV report.\u001b[0m\n",
      "\n",
      "\u001b[39mThis said, you can of course feed this command any kind of CSV data,\u001b[0m\n",
      "\u001b[39mand use dedicated flags such as --status-column, \u001b[0m\u001b[36m--body-column\u001b[0m\u001b[39m to\u001b[0m\n",
      "\u001b[39mto inform the command about your specific table.\u001b[0m\n",
      "\n",
      "\u001b[39mThe command is also able to work on glob patterns, such as: \"\u001b[0m\u001b[38;5;36mdownloaded/**/*.html\u001b[0m\u001b[39m\",\u001b[0m\n",
      "\u001b[39mand can also be fed CSV columns containing HTML content directly if\u001b[0m\n",
      "\u001b[39mrequired.\u001b[0m\n",
      "\n",
      "\u001b[38;2;250;194;43mPositional Arguments:\u001b[0m\n",
      "  \u001b[36mscraper\u001b[0m                       \u001b[39mPath to a scraper definition file, or name of a\u001b[0m\n",
      "                                \u001b[39mbuiltin scraper, e.g. \"\u001b[0m\u001b[38;5;36mtitle\u001b[0m\u001b[39m\". See the complete\u001b[0m\n",
      "                                \u001b[39mlist below.\u001b[0m\n",
      "  \u001b[36mfilename_or_filename_column\u001b[0m   \u001b[39mSingle filename to process or name of the CSV\u001b[0m\n",
      "                                \u001b[39mcolumn containing filenames when using\u001b[0m\n",
      "                                \u001b[36m-i\u001b[0m\u001b[39m/\u001b[0m\u001b[36m--input\u001b[0m\u001b[39m. Defaults to \"\u001b[0m\u001b[38;5;36mfilename\u001b[0m\u001b[39m\".\u001b[0m\n",
      "\n",
      "\u001b[38;2;250;194;43mOptional Arguments:\u001b[0m\n",
      "  \u001b[36m--body-column\u001b[0m \u001b[38;5;36mBODY_COLUMN\u001b[0m     \u001b[39mName of the CSV column containing html bodies.\u001b[0m\n",
      "                                \u001b[39mDefaults to `\u001b[0m\u001b[38;5;36mbody\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m--chunk-size\u001b[0m \u001b[38;5;36mCHUNK_SIZE\u001b[0m       \u001b[39mChunk size for multiprocessing. Defaults to `\u001b[0m\u001b[38;5;36m1\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m--encoding\u001b[0m \u001b[38;5;36mENCODING\u001b[0m           \u001b[39mName of the default encoding to use. If not\u001b[0m\n",
      "                                \u001b[39mgiven the command will infer it for you.\u001b[0m\n",
      "  \u001b[36m--encoding-column\u001b[0m \u001b[38;5;36mENCODING_COLUMN\u001b[0m\n",
      "                                \u001b[39mName of the CSV column containing file encoding.\u001b[0m\n",
      "                                \u001b[39mDefaults to `\u001b[0m\u001b[38;5;36mencoding\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m--error-column\u001b[0m \u001b[38;5;36mERROR_COLUMN\u001b[0m   \u001b[39mName of the CSV column containing a fetch error.\u001b[0m\n",
      "                                \u001b[39mDefaults to `\u001b[0m\u001b[38;5;36mfetch_error\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m-f\u001b[0m, \u001b[36m--format\u001b[0m \u001b[38;5;36m{csv,jsonl,ndjson}\u001b[0m\n",
      "                                \u001b[39mOutput format. Defaults to `\u001b[0m\u001b[38;5;36mcsv\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m-g\u001b[0m, \u001b[36m--glob\u001b[0m                    \u001b[39mWill interpret given filename as glob patterns\u001b[0m\n",
      "                                \u001b[39mto resolve if given.\u001b[0m\n",
      "  \u001b[36m-I\u001b[0m, \u001b[36m--input-dir\u001b[0m \u001b[38;5;36mINPUT_DIR\u001b[0m     \u001b[39mDirectory where the HTML files are stored.\u001b[0m\n",
      "  \u001b[36m--mimetype-column\u001b[0m \u001b[38;5;36mMIMETYPE_COLUMN\u001b[0m\n",
      "                                \u001b[39mName of the CSV column containing file mimetype.\u001b[0m\n",
      "                                \u001b[39mDefaults to `\u001b[0m\u001b[38;5;36mmimetype\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m--plural-separator\u001b[0m \u001b[38;5;36mPLURAL_SEPARATOR\u001b[0m\n",
      "                                \u001b[39mSeparator use to join lists of values when\u001b[0m\n",
      "                                \u001b[39mserializing to CSV. Defaults to `\u001b[0m\u001b[38;5;36m|\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m-p\u001b[0m, \u001b[36m--processes\u001b[0m \u001b[38;5;36mPROCESSES\u001b[0m     \u001b[39mNumber of processes to use. Defaults to roughly\u001b[0m\n",
      "                                \u001b[39mhalf of the available CPUs.\u001b[0m\n",
      "  \u001b[36m--status-column\u001b[0m \u001b[38;5;36mSTATUS_COLUMN\u001b[0m\n",
      "                                \u001b[39mName of the CSV column containing HTTP status.\u001b[0m\n",
      "                                \u001b[39mDefaults to `\u001b[0m\u001b[38;5;36mhttp_status\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m--strain\u001b[0m \u001b[38;5;36mSTRAIN\u001b[0m               \u001b[39mOptional CSS selector used to strain, i.e. only\u001b[0m\n",
      "                                \u001b[39mparse matched tags in the parsed html files in\u001b[0m\n",
      "                                \u001b[39morder to optimize performance.\u001b[0m\n",
      "  \u001b[36m--url-column\u001b[0m \u001b[38;5;36mURL_COLUMN\u001b[0m       \u001b[39mName of the CSV column containing the url.\u001b[0m\n",
      "                                \u001b[39mDefaults to `\u001b[0m\u001b[38;5;36mresolved_url\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m-s\u001b[0m, \u001b[36m--select\u001b[0m \u001b[38;5;36mSELECT\u001b[0m           \u001b[39mColumns of \u001b[0m\u001b[36m-i\u001b[0m\u001b[39m/\u001b[0m\u001b[36m--input\u001b[0m\u001b[39m CSV file to include in the\u001b[0m\n",
      "                                \u001b[39moutput (separated by `\u001b[0m\u001b[38;5;36m,\u001b[0m\u001b[39m`). Use an empty string\u001b[0m\n",
      "                                \u001b[39mif you don't want to keep anything: \u001b[0m\u001b[36m--select\u001b[0m\u001b[39m ''.\u001b[0m\n",
      "  \u001b[36m--explode\u001b[0m \u001b[38;5;36mEXPLODE\u001b[0m             \u001b[39mUse to indicate the character used to separate\u001b[0m\n",
      "                                \u001b[39mmultiple values in a single CSV cell. Defaults\u001b[0m\n",
      "                                \u001b[39mto none, i.e. CSV cells having a single values,\u001b[0m\n",
      "                                \u001b[39mwhich is usually the case.\u001b[0m\n",
      "  \u001b[36m--total\u001b[0m \u001b[38;5;36mTOTAL\u001b[0m                 \u001b[39mTotal number of items to process. Might be\u001b[0m\n",
      "                                \u001b[39mnecessary when you want to display a finite\u001b[0m\n",
      "                                \u001b[39mprogress indicator for large files given as\u001b[0m\n",
      "                                \u001b[39minput to the command.\u001b[0m\n",
      "  \u001b[36m-i\u001b[0m, \u001b[36m--input\u001b[0m \u001b[38;5;36mINPUT\u001b[0m             \u001b[39mCSV file (potentially gzipped) containing all\u001b[0m\n",
      "                                \u001b[39mthe filenames you want to process. Will consider\u001b[0m\n",
      "                                \u001b[39m`\u001b[0m\u001b[38;5;36m-\u001b[0m\u001b[39m` as stdin.\u001b[0m\n",
      "  \u001b[36m-o\u001b[0m, \u001b[36m--output\u001b[0m \u001b[38;5;36mOUTPUT\u001b[0m           \u001b[39mPath to the output file. Will consider `\u001b[0m\u001b[38;5;36m-\u001b[0m\u001b[39m` as\u001b[0m\n",
      "                                \u001b[39mstdout. If not given, results will also be\u001b[0m\n",
      "                                \u001b[39mprinted to stdout.\u001b[0m\n",
      "  \u001b[36m--refresh-per-second\u001b[0m \u001b[38;5;36mREFRESH_PER_SECOND\u001b[0m\n",
      "                                \u001b[39mNumber of times to refresh the progress bar per\u001b[0m\n",
      "                                \u001b[39msecond. Can be a float e.g. `\u001b[0m\u001b[38;5;36m0.5\u001b[0m\u001b[39m` meaning once\u001b[0m\n",
      "                                \u001b[39mevery two seconds. Use this to limit CPU usage\u001b[0m\n",
      "                                \u001b[39mwhen launching multiple commands at once.\u001b[0m\n",
      "                                \u001b[39mDefaults to `\u001b[0m\u001b[38;5;36m10\u001b[0m\u001b[39m`.\u001b[0m\n",
      "  \u001b[36m--simple-progress\u001b[0m             \u001b[39mWhether to simplify the progress bar and make it\u001b[0m\n",
      "                                \u001b[39mfit on a single line. Can be useful in terminals\u001b[0m\n",
      "                                \u001b[39mwith partial ANSI support, e.g. a Jupyter\u001b[0m\n",
      "                                \u001b[39mnotebook cell.\u001b[0m\n",
      "  \u001b[36m--silent\u001b[0m                      \u001b[39mWhether to suppress all the log and progress\u001b[0m\n",
      "                                \u001b[39mbars. Can be useful when piping.\u001b[0m\n",
      "  \u001b[36m-h\u001b[0m, \u001b[36m--help\u001b[0m                    \u001b[39mshow this help message and exit\u001b[0m\n",
      "\n",
      "\u001b[38;2;250;194;43mBuiltin scrapers:\u001b[0m\n",
      "\n",
      "\u001b[39m. \"\u001b[0m\u001b[38;5;36mcanonical\u001b[0m\u001b[39m\": scrape the \u001b[0m\u001b[38;5;36m<link rel=\"\u001b[0m\u001b[38;5;36mcanonical\u001b[0m\u001b[38;5;36m\">\u001b[0m\u001b[39m tag href if any.\u001b[0m\n",
      "\u001b[39m. \"\u001b[0m\u001b[38;5;36mmetas\u001b[0m\u001b[39m\": scrape the \u001b[0m\u001b[38;5;36m<meta>\u001b[0m\u001b[39m tags if any.\u001b[0m\n",
      "\u001b[39m. \"\u001b[0m\u001b[38;5;36mrss\u001b[0m\u001b[39m\": scrape the RSS feed urls if any.\u001b[0m\n",
      "\u001b[39m. \"\u001b[0m\u001b[38;5;36mtitle\u001b[0m\u001b[39m\": scrape the \u001b[0m\u001b[38;5;36m<title>\u001b[0m\u001b[39m tag if any.\u001b[0m\n",
      "\u001b[39m. \"\u001b[0m\u001b[38;5;36murls\u001b[0m\u001b[39m\": scrape all the relevant \u001b[0m\u001b[38;5;36m<a>\u001b[0m\u001b[39m tag href urls. Will join them\u001b[0m\n",
      "\u001b[39m    with the correct base url if \u001b[0m\u001b[36m--url-column\u001b[0m\u001b[39m was given.\u001b[0m\n",
      "\n",
      "\u001b[38;2;250;194;43mExamples:\u001b[0m\n",
      "\n",
      "\u001b[39m. Scraping a single file on disk:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape scraper.yml ./path/to/file.html\u001b[0m\n",
      "\n",
      "\u001b[39m. Scraping a `\u001b[0m\u001b[38;5;36mminet fetch\u001b[0m\u001b[39m` report:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape scraper.yml \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m report.csv \u001b[0m\u001b[36m-I\u001b[0m\u001b[36m downloaded > scraped.csv\u001b[0m\n",
      "\n",
      "\u001b[39m. Scraping a single url:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet fetch \"\u001b[0m\u001b[36mhttps://lemonde.fr\u001b[0m\u001b[36m\"\u001b[0m\u001b[36m | minet scrape scraper.yml \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m -\u001b[0m\n",
      "\n",
      "\u001b[39m. Indicating a custom filename column (named \"\u001b[0m\u001b[38;5;36mpath\u001b[0m\u001b[39m\"):\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape scraper.yml path \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m report.csv \u001b[0m\u001b[36m-I\u001b[0m\u001b[36m downloaded > scraped.csv\u001b[0m\n",
      "\n",
      "\u001b[39m. Scraping a CSV column containing HTML directly:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape scraper.yml \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m report.csv \u001b[0m\u001b[36m--body-column\u001b[0m\u001b[36m html > scraped.csv\u001b[0m\n",
      "\n",
      "\u001b[39m. Scraping a bunch of files using a glob pattern:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape scraper.yml \"\u001b[0m\u001b[38;5;36m./content/**/*.html\u001b[0m\u001b[36m\" \u001b[0m\u001b[36m--glob\u001b[0m\u001b[36m > scraped.csv\u001b[0m\n",
      "\n",
      "\u001b[39m. Scraping using a CSV file containing glob patterns:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape scraper.yml pattern \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m patterns.csv \u001b[0m\u001b[36m--glob\u001b[0m\u001b[36m > scraped.csv\u001b[0m\n",
      "\n",
      "\u001b[39m. Working on a fetch report from stdin (mind the `\u001b[0m\u001b[38;5;36m-\u001b[0m\u001b[39m`):\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet fetch url file.csv | minet scrape scraper.yml \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m - \u001b[0m\u001b[36m-I\u001b[0m\u001b[36m downloaded > scraped.csv\u001b[0m\n",
      "\n",
      "\u001b[39m. Yielding items as newline-delimited JSON (jsonl):\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape scraper.yml \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m report.csv \u001b[0m\u001b[36m--format\u001b[0m\u001b[36m jsonl > scraped.jsonl\u001b[0m\n",
      "\n",
      "\u001b[39m. Using a strainer to optimize performance:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape links-scraper.yml \u001b[0m\u001b[36m--strain\u001b[0m\u001b[36m \"\u001b[0m\u001b[38;5;36ma\u001b[0m\u001b[36m\" \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m report.csv > links.csv\u001b[0m\n",
      "\n",
      "\u001b[39m. Keeping some columns from input CSV file:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape scraper.yml \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m report.csv \u001b[0m\u001b[36m-s\u001b[0m\u001b[36m name,url > scraped.csv\u001b[0m\n",
      "\n",
      "\u001b[39m. Using a builtin scraper:\u001b[0m\n",
      "\u001b[39m    $ \u001b[0m\u001b[36mminet scrape title \u001b[0m\u001b[36m-i\u001b[0m\u001b[36m report.csv > titles.csv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../../.. && python -m minet.cli scrape -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
